{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent() # From here we generate a random user agent\n",
    "proxies = [] # Will contain proxies [ip, port]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProx():\n",
    "    proxies_req = Request('https://www.sslproxies.org/')\n",
    "    proxies_req.add_header('User-Agent', ua.random)\n",
    "    proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "    soup = BeautifulSoup(proxies_doc, 'html.parser')\n",
    "    proxies_table = soup.find(id='proxylisttable')\n",
    "         # Save proxies in the array\n",
    "    global proxies\n",
    "    proxies=[]\n",
    "    for row in proxies_table.tbody.find_all('tr'):\n",
    "        proxies.append({\n",
    "        'ip':   row.find_all('td')[0].string,\n",
    "        'port': row.find_all('td')[1].string\n",
    "      })\n",
    "    #print(len(proxies))\n",
    "    #print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getProx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "proxy_index = random.randint(0, len(proxies) - 1)\n",
    "proxy = proxies[proxy_index] \n",
    "req = Request('http://icanhazip.com')\n",
    "req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"altmetricIDS.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Altmetrics_Id']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"youtube_links\"] = \"\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(df['Altmetrics_Id'])\n",
    "ids[5255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_param  = {\"key\" : \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alt_data = requests.get(\"https://api.altmetric.com/v1/id/1969481\", alt_param )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data = json.loads(all_alt_data.text)\n",
    "alt_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_links = []\n",
    "html = requests.get('https://www.altmetric.com/details/' + str(403270) + '/video', alt_param).text\n",
    "bs = BeautifulSoup(html)\n",
    "possible_links = bs.find_all('a')\n",
    "for link in possible_links:\n",
    "    if link.has_attr('href') and 'youtube' in link.attrs['href']:\n",
    "        y_links.append(link.attrs['href'])\n",
    "y_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common import exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome = webdriver.Chrome('C:/ProgramData/chocolatey/bin/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome.get('https://www.altmetric.com/details/403270/video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages(main_link):\n",
    "    myset= set()\n",
    "    chrome.get(main_link)\n",
    "    elements = chrome.find_elements_by_xpath(\"//a[@href]\")\n",
    "    for elem in elements:\n",
    "        if \"video/page:\" in elem.get_attribute(\"href\"):\n",
    "            myset.add(elem.get_attribute(\"href\"))\n",
    "    return myset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_ids(link):\n",
    "    y_links = []\n",
    "    chrome.get(link)\n",
    "    elements = chrome.find_elements_by_xpath(\"//a[@href]\")\n",
    "    for elem in elements:\n",
    "        if \"youtube\" in elem.get_attribute(\"href\"):\n",
    "            y_links.append(elem.get_attribute(\"href\")[32:])\n",
    "    return y_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('youtube.csv', 'w')\n",
    "for i in range(len(ids)):\n",
    "    try:\n",
    "        getProx()\n",
    "        proxy_index = random.randint(0, len(proxies) - 1)\n",
    "        proxy = proxies[proxy_index] \n",
    "        req = Request('http://icanhazip.com')\n",
    "        req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')\n",
    "        sleep(0.5)\n",
    "        y_links = get_youtube_ids('https://www.altmetric.com/details/' + str(ids[i]) + '/video')\n",
    "        myset = get_all_pages('https://www.altmetric.com/details/' + str(ids[i]) + '/video')\n",
    "        for link in myset:\n",
    "            y_links.extend(get_youtube_ids(link))\n",
    "        df.at[i, 'youtube_links'] = y_links\n",
    "        file.write(str(ids[i]) + \",\")\n",
    "        file.write(\" \".join(y_links) + \"\\n\")\n",
    "        \n",
    "    except exceptions.StaleElementReferenceException:\n",
    "        sleep(0.5)\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        my_ip = urlopen(req).read().decode('utf8')        \n",
    "        print('#' + str(n) + ': ' + my_ip)\n",
    "\n",
    "    except:  \n",
    "        # If error, delete this proxy and find another one\n",
    "        #global proxies\n",
    "        del proxies[proxy_index]\n",
    "        print('proxy deleted')\n",
    "        proxy_index = random.randint(0, len(proxies) - 1)\n",
    "        proxy = proxies[proxy_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[4, 'youtube_links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"youtube_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data['details_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_video(alt_id):\n",
    "    try:\n",
    "        response = requests.get(\"https://api.altmetric.com/v1/id/\" + str(alt_id))\n",
    "        if 'video' in dict(json.loads(response.content)):\n",
    "            return dict(json.loads(response.content))['video']\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\"key\" : \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_content = \"https://www.googleapis.com/youtube/v3/videos?part=snippet&key=&id=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids[7132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alt_id in ids[7133:7433]:\n",
    "    mylist = []\n",
    "    alt_req = requests.get(\"https://api.altmetric.com/v1/id/\" + str(alt_id))\n",
    "    alt_rs= json.loads(alt_req.text)\n",
    "    mylist.append({str(alt_id) : alt_rs})\n",
    "    y_links = []\n",
    "    html = requests.get('https://www.altmetric.com/details/' + str(alt_id) + '/video').text\n",
    "    bs = BeautifulSoup(html)\n",
    "    possible_links = bs.find_all('a')\n",
    "    for link in possible_links:\n",
    "        if link.has_attr('href') and 'youtube' in link.attrs['href']:\n",
    "            y_links.append(link.attrs['href'])\n",
    "    youtube_data = []\n",
    "    for link in y_links:\n",
    "        param[\"id\"] = link[32:]\n",
    "        response_statistics = requests.get(url, param)\n",
    "        print(response_statistics.status_code)\n",
    "        json_statistics = json.loads(response_statistics.text)\n",
    "        youtube_data.append(json_statistics)\n",
    "    mylist.append({'youtube':youtube_data})\n",
    "    file = open(str(alt_id) + \".txt\", \"w+\")\n",
    "    json.dump(mylist, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:/Users/srika/Youtube_dataSet\"\n",
    "files = os.listdir(path)\n",
    "len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    files[i] = os.path.abspath(path + \"/\" + files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"C:\\\\Users\\\\srika\\\\Youtube_dataSet\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame()\n",
    "col = ['cited_by_fbwalls_count', 'cited_by_feeds_count', 'cited_by_gplus_count', 'cited_by_msm_count', \n",
    "       'cited_by_posts_count', 'cited_by_rdts_count', 'cited_by_tweeters_count', 'cited_by_videos_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['viewCount', 'likeCount',  'dislikeCount', 'favoriteCount','commentCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for file in files:\n",
    "    json_file = open(file, \"r\")\n",
    "    data = json.load(json_file)\n",
    "    df_.loc[i, \"altmetric_id\"] = file[31:-4]\n",
    "    for c in col:\n",
    "        try:\n",
    "            df_.loc[i, c] = data[0][file[31:-4]][c]\n",
    "        except KeyError:\n",
    "            df_.loc[i, c] = 0\n",
    "    i += 1\n",
    "    json_file.close\n",
    "    \n",
    "df_.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos = pd.DataFrame(columns = stats, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files[:2])):\n",
    "    json_file = open(files[i], \"r\")\n",
    "    data = json.load(json_file)\n",
    "    df_videos.loc[i, 'altmetric_id'] = files[i][31:-4]\n",
    "    for stat in stats:\n",
    "        mylist = []\n",
    "        for j in range(len(data[1]['youtube'])):\n",
    "            try:\n",
    "                mylist.append(int(data[1]['youtube'][j]['items'][0]['statistics'][stat]))\n",
    "            except IndexError:\n",
    "                mylist.append(0)\n",
    "            except KeyError:\n",
    "                mylist.append(0)\n",
    "        print(stat, mylist)\n",
    "        df_videos.at[i, stat] = mylist\n",
    "    json_file.close()\n",
    "df_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv(\"altmetrics_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.to_csv(\"videos.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_link = url_statistics +\"GSqdhz--yR8\"\n",
    "rs_st = requests.get(y_link)\n",
    "rs_st_d = json.loads(rs_st.text)\n",
    "rs_st_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_content = url_content + \"GSqdhz--yR8\"\n",
    "rs_ct = requests.get(y_link)\n",
    "rs_ct_d = json.loads(rs_st.text)\n",
    "rs_ct_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_columns = ['cited_by_fbwalls_count', 'cited_by_gplus_count', 'cited_by_msm_count', 'cited_by_posts_count', \n",
    "                        'cited_by_tweeters_count', 'cited_by_videos_count', 'cited_by_wikipedia_count', \n",
    "                        'cited_by_accounts_count',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['title', 'doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ids[:100])):\n",
    "    rs = requests.get(\"https://api.altmetric.com/v1/id/\" + str(ids[i]))\n",
    "    if (rs.status_code == 200):\n",
    "        rs_json = json.loads(rs.text)\n",
    "        for col in cols:\n",
    "            try:\n",
    "                df.loc[i, col] = rs_json[col]\n",
    "            except KeyError:\n",
    "                df.loc[i, col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:100].to_csv(path_or_buf = \".\\\\sample_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_comments = \"https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=_qUhIZcxNvE&key=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.loads(response.text)\n",
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data[\"items\"][2]['snippet']['topLevelComment']['snippet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = 0\n",
    "dc = 0\n",
    "for j in len(json_data['items']):\n",
    "    lc + = json_data['items'][j][\"likeCount\"]\n",
    "    dc + = json_data['items'][j][\"likeCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add author information\n",
    "def add_all_info(alt_id):\n",
    "    # create the dict\n",
    "    result = defaultdict(dict)\n",
    "    try:\n",
    "        # make the request\n",
    "        response = requests.get(\"https://api.altmetric.com/v1/id/\" + str(alt_id))\n",
    "\n",
    "        # add the values to the dict\n",
    "        if 'authors' in dict(json.loads(response.content)):\n",
    "            result['author_count'] = len(dict(json.loads(response.content))['authors'])\n",
    "            result['authors'] = ', '.join(dict(json.loads(response.content))['authors'])\n",
    "        else:\n",
    "            result['author_count'] = ''\n",
    "            result['authors'] = ''\n",
    "\n",
    "         # add the values to the dict\n",
    "        if 'doi' in dict(json.loads(response.content)):\n",
    "            result['doi'] = dict(json.loads(response.content))['doi']\n",
    "        else:\n",
    "            result['doi'] = ''\n",
    "\n",
    "        # add the abstract\n",
    "        if 'abstract' in dict(json.loads(response.content)):\n",
    "            result['abstract'] = dict(json.loads(response.content))['abstract']\n",
    "        else:\n",
    "            result['abstract'] = ''\n",
    "\n",
    "        # add the pubdate\n",
    "        if 'published_on' in dict(json.loads(response.content)):\n",
    "            result['pub_date'] = dict(json.loads(response.content))['published_on']\n",
    "        else:\n",
    "            result['pub_date'] = ''\n",
    "\n",
    "        # add the twitter posts count as of 2018/19\n",
    "        if 'cited_by_tweeters_count' in dict(json.loads(response.content)):\n",
    "            result['twitter_count_y18'] = dict(json.loads(response.content))['cited_by_tweeters_count']\n",
    "        else:\n",
    "            result['twitter_count_y18'] = ''\n",
    "\n",
    "        # add the title of the article\n",
    "        if 'title' in dict(json.loads(response.content)):\n",
    "            result['title'] =  dict(json.loads(response.content))['title']\n",
    "        else:\n",
    "            result['title'] = ''\n",
    "\n",
    "        # return the result\n",
    "        return result\n",
    "    except:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = add_all_info(702709 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ = requests.get(\"https://api.altmetric.com/v1/id/\" + str(1027520))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not Found'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5ff58c2110a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrs_js\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \"\"\"\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "rs_js = json.loads(rs_.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'doi', 'pmid', 'tq', 'altmetric_jid', 'issns', 'journal', 'cohorts', 'abstract', 'abstract_source', 'context', 'authors', 'type', 'altmetric_id', 'schema', 'is_oa', 'publisher_subjects', 'cited_by_fbwalls_count', 'cited_by_feeds_count', 'cited_by_gplus_count', 'cited_by_msm_count', 'cited_by_posts_count', 'cited_by_rdts_count', 'cited_by_tweeters_count', 'cited_by_videos_count', 'cited_by_wikipedia_count', 'cited_by_accounts_count', 'last_updated', 'score', 'history', 'url', 'added_on', 'published_on', 'subjects', 'readers', 'readers_count', 'images', 'details_url'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_js.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vegetarians, those who avoid meat, and vegans, additionally avoiding dairy and eggs, represent 5% and 2%, respectively, of the US population. The aim of this review is to assess the effects of vegetarian diets, particularly strict vegetarian diets (i.e., vegans) on health and disease outcomes. We summarized available evidence from three prospective cohorts of Adventists in North America: Adventist Mortality Study, Adventist Health Study, and Adventist Health Study-2. Non-vegetarian diets were compared to vegetarian dietary patterns (i.e., vegan and lacto-ovo-vegetarian) on selected health outcomes. Vegetarian diets confer protection against cardiovascular diseases, cardiometabolic risk factors, some cancers and total mortality. Compared to lacto-ovo-vegetarian diets, vegan diets seem to offer additional protection for obesity, hypertension, type-2 diabetes, and cardiovascular mortality. Males experience greater health benefits than females. Limited prospective data is available on vegetarian diets and body weight change. Large randomized intervention trials on the effects of vegetarian diet patterns on neurological and cognitive functions, obesity, diabetes, and other cardiovascular outcomes are warranted to make meaningful recommendations.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_js['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open(\"temp.txt\", 'w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(rs_js, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_js['cited_by_accounts_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_js['cited_by_videos_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_statistics = requests.get(url_statistics)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_statistics = json.loads(response_statistics.text)\n",
    "json_statistics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_statistics['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = {\"LOGIN_INFO\": \"AFmmF2swRQIhAIEmerJsgJPTtDcSlfROZB4nA7CDB8gni2t6wihB-r6wAiArbacirmZ0s6ofJZ5ORMIfI9dKOj-pXVAHPIh22PO6qQ:QUQ3MjNmd0VYNm1TVmhVUmdRM2Z5U3RGdy1tQTRnUkN4clJIdTJiakVSYmdvd0F5bVNKRDNtZndtemtsM1FBQkM3NzduRHFKaDhVdGlvelQwM3UtX0xKc2tVemRLT3NSTGgyUkhwdkJLbHZHWlQyZUpVaXcyb1VfdnR2c1pESXM2eWlUMGczbVl1Y0VPZTVHQklJMFdRQnNWOWZSX2pVMU16ZkRPN21NS2lHdmhReDNQQWVYZERJ\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "you_req = requests.get(\"https://www.youtube.com/watch?v=zAi6Qe0qiIs\", params = param, cookies=cookies).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(you_req)\n",
    "possible_links = bs.find_all('script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "possible_links[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from urllib.request import Request, urlopen\n",
    "from fake_useragent import UserAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C://Users//srika//Youtube-Videos//youtube.csv\")\n",
    "df.columns = [\"Altmetric_id\", \"youtube_ids\"]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent() # From here we generate a random user agent\n",
    "proxies = [] # Will contain proxies [ip, port]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProx():\n",
    "    proxies_req = Request('https://www.sslproxies.org/')\n",
    "    proxies_req.add_header('User-Agent', ua.random)\n",
    "    proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "    soup = BeautifulSoup(proxies_doc, 'html.parser')\n",
    "    proxies_table = soup.find(id='proxylisttable')\n",
    "         # Save proxies in the array\n",
    "    global proxies\n",
    "    proxies=[]\n",
    "    for row in proxies_table.tbody.find_all('tr'):\n",
    "        proxies.append({\n",
    "        'ip':   row.find_all('td')[0].string,\n",
    "        'port': row.find_all('td')[1].string\n",
    "      })\n",
    "    #print(len(proxies))\n",
    "    #print(proxies)\n",
    "    \n",
    "def random_proxy():\n",
    "    return random.randint(0, len(proxies) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve latest proxies\n",
    "proxies_req = Request('https://www.sslproxies.org/')\n",
    "proxies_req.add_header('User-Agent', ua.random)\n",
    "proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "\n",
    "soup = BeautifulSoup(proxies_doc, 'html.parser')\n",
    "proxies_table = soup.find(id='proxylisttable')\n",
    "\n",
    "# Save proxies in the array\n",
    "for row in proxies_table.tbody.find_all('tr'):\n",
    "    proxies.append({\n",
    "            'ip':   row.find_all('td')[0].string,\n",
    "            'port': row.find_all('td')[1].string\n",
    "            })\n",
    "    print(len(proxies))\n",
    "    print(proxies)\n",
    "\n",
    "\n",
    "# Choose a random proxy\n",
    "proxy_index = random_proxy()\n",
    "proxy = proxies[proxy_index] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--user-data-dir=chrome-data\")\n",
    "chrome = webdriver.Chrome('C:/ProgramData/chocolatey/bin/chromedriver.exe', options=chrome_options)\n",
    "chrome_options.add_argument(\"user-data-dir=chrome-data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in list(df.iterrows())[7433:7434]:\n",
    "    try:\n",
    "        proxy_index = random.randint(0, len(proxies) - 1)\n",
    "        proxy = proxies[proxy_index] \n",
    "        req = Request('http://icanhazip.com')\n",
    "        req.set_proxy(proxy['ip'] + ':' + proxy['port'], 'http')\n",
    "\n",
    "        alt_id = row[\"Altmetric_id\"]\n",
    "\n",
    "        mylist=[]\n",
    "        alt_req = requests.get(\"https://api.altmetric.com/v1/id/\" + str(alt_id))\n",
    "        alt_rs= json.loads(alt_req.text)\n",
    "        #print(alt_req.text)\n",
    "        mylist.append({str(alt_id):alt_rs})\n",
    "\n",
    "        y_ids = row['youtube_ids'].split()\n",
    "        y_list = []\n",
    "        for y_id in y_ids:\n",
    "            y_data = {}\n",
    "            y_data['videoId'] = y_id\n",
    "            chrome.get(\"https://www.youtube.com/watch?v=\" + y_id)\n",
    "            chrome.implicitly_wait(20)\n",
    "            print(\"chrome\")\n",
    "            channel_id = chrome.find_element_by_xpath('//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "            print(\"yes\")\n",
    "            c_id = channel_id.get_attribute('href')\n",
    "            y_data[\"channelId\"] = c_id[32:]\n",
    "            print(\"ID\")\n",
    "            v_title =chrome.find_element_by_xpath('//h1[@class=\"title style-scope ytd-video-primary-info-renderer\"]')\n",
    "            y_data[\"title\"] = v_title.text\n",
    "            print(\"title\")\n",
    "            v_description =  chrome.find_element_by_xpath('//div[@id = \"description\" and @class=\"style-scope ytd-video-secondary-info-renderer\"]')\n",
    "            y_data[\"description\"] = v_description.text\n",
    "            print(\"description\")\n",
    "            elements = chrome.find_elements_by_xpath(\n",
    "                    '//yt-formatted-string[@id= \"text\" and @class = \"style-scope ytd-toggle-button-renderer style-text\"]')\n",
    "            for element in elements:\n",
    "                if \"likes\" in element.get_attribute(\"aria-label\"):\n",
    "                    likes = int(element.text)\n",
    "                    y_data['likeCount'] = likes\n",
    "                if \"dislikes\" in element.get_attribute(\"aria-label\"):\n",
    "                    dislikes = int(element.text)\n",
    "                    y_data[\"dislikeCount\"] = dislikes\n",
    "            print(\"likes and dislikes\")\n",
    "            v_count = chrome.find_element_by_xpath('//div[@id = \"count\" and @class=\"style-scope ytd-video-primary-info-renderer\"]')\n",
    "            count = int(v_count.text.replace(\",\", \"\").split()[0])\n",
    "            y_data[\"viewCount\"] = count\n",
    "            print(\"viewCount\")\n",
    "            chrome.implicitly_wait(20)\n",
    "            n_comments =  chrome.find_element_by_xpath('//yt-formatted-string[@class=\"count-text style-scope ytd-comments-header-renderer\"]')\n",
    "            n_comm = int(n_comments.text.replace(\",\", \"\").split()[0])\n",
    "            y_data[\"commentCount\"] = n_comm\n",
    "            print(\"commentCount\")\n",
    "            y_list.append(y_data)\n",
    "        mylist.append({\"youtube\":y_list})\n",
    "        file = open(str(alt_id) + \".txt\", \"w+\")\n",
    "        json.dump(mylist, file)\n",
    "        file.close()\n",
    "\n",
    "    except exceptions.StaleElementReferenceException:\n",
    "        print('Stale')\n",
    "        alt_id = row[\"Altmetric_id\"]\n",
    "        file = open(str(alt_id) + \".txt\", \"w+\")\n",
    "        file.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comments = chrome.find_element_by_xpath('//yt-formatted-string[@class=\"count-text style-scope ytd-comments-header-renderer\"]')\n",
    "n_comm = int(n_comments.text.replace(\",\", \"\").split()[0])\n",
    "n_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = chrome.find_elements_by_xpath(\n",
    "                    '//yt-formatted-string[@id= \"text\" and @class = \"style-scope ytd-toggle-button-renderer style-text\"]')\n",
    "for element in elements:\n",
    "    if \"dislikes\" or \"dislike\" in element.get_attribute(\"aria-label\"):\n",
    "        print(element.text)\n",
    "        continue\n",
    "    if \"like\" or \"likes\" in element.get_attribute(\"aria-label\"):\n",
    "        print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_description =  chrome.find_element_by_xpath('//div[@id = \"description\" and @class=\"style-scope ytd-video-secondary-info-renderer\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_description.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_likes =  wait.until(EC.presence_of_element_located(\n",
    "                                         (By.CSS_SELECTOR,\"div#top-level-buttons yt-formatted-string\"))).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =chrome.find_element_by_xpath('//div[@id = \"count\" and @class=\"style-scope ytd-video-primary-info-renderer\"]')\n",
    "count.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = int(count.replace(\",\", \"\").split()[0])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comments = chrome.find_element_by_xpath('//h2[@id=\"count\" and @class=\"style-scope ytd-comments-header-renderer\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comments.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comm = int(n_comments.text.replace(\",\", \"\").split()[0])\n",
    "n_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = chrome.find_element_by_xpath('//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_id = channel_id.get_attribute('href')\n",
    "c_id[32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_elements = chrome.find_elements_by_xpath('//ytd-comments')\n",
    "for c in comment_elements:\n",
    "    for element in c.find_elements_by_tag_name('yt-formatted-string'):\n",
    "        print (element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('https://www.youtube.com/channel/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cat = chrome.find_element_by_xpath('//div[@id=\"content\" and @class = \"style-scope ytd-metadata-row-renderer\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cat.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
