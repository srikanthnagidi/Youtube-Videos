# -*- coding: utf-8 -*-
"""predicting_subjects_multilabel_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DzKSXsjvux0SBdj44y6W6La8aZGp9NY0
"""

from google.colab import files
files.upload()

import pandas as pd
import ast
df = pd.read_csv("combined_with_news_n_blogs.csv", header=0)

subjects = ['Agricultural and Biological Sciences',
 'Arts and Humanities',
 'Biochemistry, Genetics and Molecular Biology',
 'Business, Management and Accounting',
 'Chemical Engineering',
 'Chemistry',
 'Computer Science',
 'Decision Sciences',
 'Dentistry',
 'Earth and Planetary Sciences',
 'Economics, Econometrics and Finance',
 'Energy',
 'Engineering',
 'Environmental Science',
 'General',
 'Health Professions',
 'Health Sciences',
 'Immunology and Microbiology',
 'Life Sciences',
 'Materials Science',
 'Mathematics',
 'Medicine',
 'Neuroscience',
 'Nursing',
 'Pharmacology, Toxicology and Pharmaceutics',
 'Physical Sciences',
 'Physics and Astronomy',
 'Psychology',
 'Social Sciences',
 'Veterinary']

for sub in subjects:
  df[sub] = 0

len(df)

df = df[df.scopus_subjects != '[]']
len(df)

for index, row in df.iterrows():
  for sub in subjects:
    if sub in ast.literal_eval(row['scopus_subjects']):
      df.loc[index, sub] = 1
  print (index)

import numpy as np
def convert_si_to_number(x):
    total_stars = 0
    if 'K' in x:
        if len(x) > 1:
            total_stars = float(x.replace('K', '')) * 1000 # convert k to a thousand
    if 'M' in x:
        if len(x) > 1:
            total_stars = float(x.replace('M', '')) * 1000000 # convert M to a million
    return int(total_stars)

for index, row in df.iterrows():
    if (row['views'] =='0'):
        continue
    df.loc[index, "views"] = pd.to_numeric(ast.literal_eval(row['views']), errors='coerce').mean()
    df.loc[index, "likes"] = pd.to_numeric(ast.literal_eval(row['likes']), errors='coerce').mean()
    df.loc[index, "dislikes"] = pd.to_numeric(ast.literal_eval(row['dislikes']), errors='coerce').mean()
    df.loc[index, "CommentCount"] = pd.to_numeric(ast.literal_eval(row['CommentCount']), errors='coerce').mean()
    df.loc[index, "subno"] = np.mean([convert_si_to_number(number) for number in ast.literal_eval(row['subno'])])
    print(index)

data = df.loc[:,['cited_by_gplus_count', 'cited_by_wikipedia_count', 'cited_by_rdts_count',
       'Video mentions', 'cited_by_fbwalls_count','cited_by_tweeters_count','views', 'likes', 'dislikes', 'CommentCount', 'subno', 'Number of Dimensions citations', 'cited_by_feeds_count',
       'cited_by_msm_count', 'cited_by_posts_count', 'Response_Rate', 'News mentions', 'Blog mentions'] ]
target = df.loc[:, subjects]

data.fillna(0, inplace=True)

!pip install scikit-multilearn

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from skmultilearn.problem_transform import BinaryRelevance

traindata,testdata,traintarget,testtarget = train_test_split(data, target, test_size=0.25)

#feature scaling
sc = StandardScaler()
traindata = sc.fit_transform(traindata)
testdata = sc.transform(testdata)

nb_clf = MultinomialNB()
lr = LogisticRegression()
mn = MultinomialNB()
random = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)

from sklearn.metrics import hamming_loss
def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):
    '''
    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case
    http://stackoverflow.com/q/32239577/395857
    '''
    acc_list = []
    for i in range(y_true.shape[0]):
        set_true = set( np.where(y_true[i])[0] )
        set_pred = set( np.where(y_pred[i])[0] )
        tmp_a = None
        if len(set_true) == 0 and len(set_pred) == 0:
            tmp_a = 1
        else:
            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )
        acc_list.append(tmp_a)
    return np.mean(acc_list)

def print_score(y_pred, clf):
    print("Clf: ", clf.__class__.__name__)
    print("Hamming loss: {}".format(hamming_loss(testdata, y_pred)))
    print("Hamming score: {}".format(hamming_score(testdata, y_pred)))
    print("---")

classifier = BinaryRelevance(
    classifier = lr,
    require_dense = [False, True]
)
classifier.fit(traindata, traintarget)
y_pred = classifier.predict(testdata)

import sklearn.metrics as metrics

br_f1=metrics.f1_score(testtarget, y_pred, average='micro')
br_hamm=metrics.hamming_loss(testtarget,y_pred)
print('Binary Relevance F1-score:',round(br_f1,3))
print('Binary Relevance Hamming Loss:',round(br_hamm,3))

from skmultilearn.problem_transform import LabelPowerset

classifier = LabelPowerset(
    classifier = random,
    require_dense = [False, True]
)

classifier.fit(traindata, traintarget)

y_pred_random = classifier.predict(testdata)
lp_f1=metrics.f1_score(testtarget, y_pred_random, average='micro')
lp_hamm=metrics.hamming_loss(testtarget, y_pred_random)
print('Label Powerset F1-score:',round(lp_f1,3))
print('Label Powerset Hamming Loss:',round(lp_hamm,3))

metrics.accuracy_score(testtarget, y_pred_random,normalize=True, sample_weight=None)

clf = OneVsRestClassifier(lr)
clf.fit(traindata, traintarget)
y_pred_lr = clf.predict(testdata)

lp_f1=metrics.f1_score(testtarget, y_pred_lr, average='micro')
lp_hamm=metrics.hamming_loss(testtarget, y_pred_lr)
print('Label Powerset F1-score:',round(lp_f1,3))
print('Label Powerset Hamming Loss:',round(lp_hamm,3))

target.head()