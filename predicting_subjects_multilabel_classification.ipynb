{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1lQct1H5crz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv(\"combined_with_news_n_blogs.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPEbXSTxecTy"
   },
   "outputs": [],
   "source": [
    "subjects = {'Agricultural and Biological Sciences':'Life Sciences',\n",
    " 'Arts and Humanities' : 'Life Sciences' ,\n",
    " 'Biochemistry, Genetics and Molecular Biology':'Life Sciences',\n",
    " 'Business, Management and Accounting' : 'Management Sciences',\n",
    " 'Chemical Engineering' : 'Applied Sciences',\n",
    " 'Chemistry' : 'Applied Sciences',\n",
    " 'Computer Science' : 'Applied Sciences',\n",
    " 'Decision Sciences' : 'Management Sciences',\n",
    " 'Dentistry' : 'Life Sciences',\n",
    " 'Earth and Planetary Sciences': 'Applied Sciences',\n",
    " 'Economics, Econometrics and Finance' : 'Management Sciences',\n",
    " 'Energy' : 'Applied Sciences',\n",
    " 'Engineering':'Applied Sciences',\n",
    " 'Environmental Science' : 'Environmental Sciences',\n",
    " 'General':'General',\n",
    " 'Health Professions' : 'Health Sciences',\n",
    " 'Health Sciences' : 'Health Sciences',\n",
    " 'Immunology and Microbiology' : 'Life Sciences',\n",
    " 'Life Sciences' : 'Life Sciences',\n",
    " 'Materials Science' : 'Applied Sciences',\n",
    " 'Mathematics' : 'Applied Sciences',\n",
    " 'Medicine' : 'Life Sciences',\n",
    " 'Neuroscience' : 'Life Sciences',\n",
    " 'Nursing' : 'Health Sciences',\n",
    " 'Pharmacology, Toxicology and Pharmaceutics' : 'Life Sciences',\n",
    " 'Physical Sciences' : 'Applied Sciences',\n",
    " 'Physics and Astronomy' : 'Applied Sciences',\n",
    " 'Psychology' : 'Life Sciences',\n",
    " 'Social Sciences' : 'Social Sciences',\n",
    " 'Veterinary' : 'Life Sciences'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "1dSG9eCwU0oa",
    "outputId": "5f6e5e3f-73ec-48c7-fad6-7d1205476072"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Applied Sciences',\n",
       " 'Environmental Sciences',\n",
       " 'General',\n",
       " 'Health Sciences',\n",
       " 'Life Sciences',\n",
       " 'Management Sciences',\n",
       " 'Social Sciences'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(subjects.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5yu5urLigEHA",
    "outputId": "9b95d8ab-1fb6-4b09-bb7c-5013494d08aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81886"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.scopus_subjects != '[]']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvwRAlTKezrR"
   },
   "outputs": [],
   "source": [
    "for sub in set(subjects.values()):\n",
    "    df[sub] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9eC27WT_XnKR",
    "outputId": "78e7e9aa-0c5d-428b-8a4d-be9a6506e366"
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    for sub in subjects.keys():\n",
    "        if sub in ast.literal_eval(row['scopus_subjects']):\n",
    "            df.loc[index, subjects[sub]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>subname</th>\n",
       "      <th>subno</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>description</th>\n",
       "      <th>...</th>\n",
       "      <th>News mentions</th>\n",
       "      <th>Blog mentions</th>\n",
       "      <th>scopus_subjects</th>\n",
       "      <th>Applied Sciences</th>\n",
       "      <th>Health Sciences</th>\n",
       "      <th>Environmental Sciences</th>\n",
       "      <th>General</th>\n",
       "      <th>Management Sciences</th>\n",
       "      <th>Life Sciences</th>\n",
       "      <th>Social Sciences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['0']</td>\n",
       "      <td>['0.0']</td>\n",
       "      <td>['0.0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['TaEcwJ53xKs']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['Health Sciences', 'Social Sciences', 'Psycho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['2304', '154']</td>\n",
       "      <td>['312.0', '1.0']</td>\n",
       "      <td>['2.0', '0.0']</td>\n",
       "      <td>['19', '0']</td>\n",
       "      <td>['KLLZcaqjQWI', 'ytB1r_SOsbA']</td>\n",
       "      <td>['Para Que Serve uma Barata Ciborgue? (com Sup...</td>\n",
       "      <td>['Canal Cura Quântica', 'Bruno Rezende Souza']</td>\n",
       "      <td>['14K', '310']</td>\n",
       "      <td>['Published on Mar 13, 2018', 'Published on No...</td>\n",
       "      <td>['O que uma barata ciborgue pode mudar na sua ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>['General']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['8821']</td>\n",
       "      <td>['8.0']</td>\n",
       "      <td>['3.0']</td>\n",
       "      <td>['0']</td>\n",
       "      <td>['3wg14z5O9Ug']</td>\n",
       "      <td>['First Person Experience of Body Transfer']</td>\n",
       "      <td>['Mel Slater']</td>\n",
       "      <td>['441']</td>\n",
       "      <td>['Published on Mar 16, 2014']</td>\n",
       "      <td>['This shows the setup for a full body ownersh...</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>['Biochemistry, Genetics and Molecular Biology...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>['General']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['59312', '1522', '3700', '0', '86310', '6324'...</td>\n",
       "      <td>['2429.0', '15.0', '78.0', '0.0', '4662.0', '1...</td>\n",
       "      <td>['55.0', '0.0', '2.0', '0.0', '93.0', '10.0', ...</td>\n",
       "      <td>['221', '5', '7', '0', '329', '12', '741', '25...</td>\n",
       "      <td>['qoc573egMfU', 'C0wNzruReo0', 'gj4y8sJoyqE', ...</td>\n",
       "      <td>['LE SPORT qui MARCHE VRAIMENT (pour maigrir)'...</td>\n",
       "      <td>['Le Fasting', '#다이어트', 'Hockey Training', '0'...</td>\n",
       "      <td>['227K', '705', '33K', '0', '0', '5.4K', '907K...</td>\n",
       "      <td>['Published on Mar 29, 2019', 'Published on No...</td>\n",
       "      <td>['Pour rejoindre la Fast Gym (-60% en  passant...</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>10</td>\n",
       "      <td>['Medicine', 'Health Professions', 'Health Sci...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               views  \\\n",
       "0                                              ['0']   \n",
       "1                                    ['2304', '154']   \n",
       "2                                           ['8821']   \n",
       "3                                                 []   \n",
       "4  ['59312', '1522', '3700', '0', '86310', '6324'...   \n",
       "\n",
       "                                               likes  \\\n",
       "0                                            ['0.0']   \n",
       "1                                   ['312.0', '1.0']   \n",
       "2                                            ['8.0']   \n",
       "3                                                 []   \n",
       "4  ['2429.0', '15.0', '78.0', '0.0', '4662.0', '1...   \n",
       "\n",
       "                                            dislikes  \\\n",
       "0                                            ['0.0']   \n",
       "1                                     ['2.0', '0.0']   \n",
       "2                                            ['3.0']   \n",
       "3                                                 []   \n",
       "4  ['55.0', '0.0', '2.0', '0.0', '93.0', '10.0', ...   \n",
       "\n",
       "                                        CommentCount  \\\n",
       "0                                              ['0']   \n",
       "1                                        ['19', '0']   \n",
       "2                                              ['0']   \n",
       "3                                                 []   \n",
       "4  ['221', '5', '7', '0', '329', '12', '741', '25...   \n",
       "\n",
       "                                                link  \\\n",
       "0                                    ['TaEcwJ53xKs']   \n",
       "1                     ['KLLZcaqjQWI', 'ytB1r_SOsbA']   \n",
       "2                                    ['3wg14z5O9Ug']   \n",
       "3                                                 []   \n",
       "4  ['qoc573egMfU', 'C0wNzruReo0', 'gj4y8sJoyqE', ...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                              ['0']   \n",
       "1  ['Para Que Serve uma Barata Ciborgue? (com Sup...   \n",
       "2       ['First Person Experience of Body Transfer']   \n",
       "3                                                 []   \n",
       "4  ['LE SPORT qui MARCHE VRAIMENT (pour maigrir)'...   \n",
       "\n",
       "                                             subname  \\\n",
       "0                                              ['0']   \n",
       "1     ['Canal Cura Quântica', 'Bruno Rezende Souza']   \n",
       "2                                     ['Mel Slater']   \n",
       "3                                                 []   \n",
       "4  ['Le Fasting', '#다이어트', 'Hockey Training', '0'...   \n",
       "\n",
       "                                               subno  \\\n",
       "0                                              ['0']   \n",
       "1                                     ['14K', '310']   \n",
       "2                                            ['441']   \n",
       "3                                                 []   \n",
       "4  ['227K', '705', '33K', '0', '0', '5.4K', '907K...   \n",
       "\n",
       "                                             pubdate  \\\n",
       "0                                              ['0']   \n",
       "1  ['Published on Mar 13, 2018', 'Published on No...   \n",
       "2                      ['Published on Mar 16, 2014']   \n",
       "3                                                 []   \n",
       "4  ['Published on Mar 29, 2019', 'Published on No...   \n",
       "\n",
       "                                         description  ... News mentions  \\\n",
       "0                                              ['0']  ...             4   \n",
       "1  ['O que uma barata ciborgue pode mudar na sua ...  ...             0   \n",
       "2  ['This shows the setup for a full body ownersh...  ...             6   \n",
       "3                                                 []  ...             0   \n",
       "4  ['Pour rejoindre la Fast Gym (-60% en  passant...  ...            73   \n",
       "\n",
       "   Blog mentions                                    scopus_subjects  \\\n",
       "0              4  ['Health Sciences', 'Social Sciences', 'Psycho...   \n",
       "1              9                                        ['General']   \n",
       "2              9  ['Biochemistry, Genetics and Molecular Biology...   \n",
       "3              2                                        ['General']   \n",
       "4             10  ['Medicine', 'Health Professions', 'Health Sci...   \n",
       "\n",
       "   Applied Sciences  Health Sciences  Environmental Sciences  General  \\\n",
       "0                 0                1                       0        0   \n",
       "1                 0                0                       0        1   \n",
       "2                 0                1                       0        0   \n",
       "3                 0                0                       0        1   \n",
       "4                 0                1                       0        0   \n",
       "\n",
       "   Management Sciences  Life Sciences Social Sciences  \n",
       "0                    0              1               1  \n",
       "1                    0              0               0  \n",
       "2                    0              1               0  \n",
       "3                    0              0               0  \n",
       "4                    0              1               0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZgKtHLjiTI-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert_si_to_number(x):\n",
    "    total_stars = 0\n",
    "    if 'K' in x:\n",
    "        if len(x) > 1:\n",
    "            total_stars = float(x.replace('K', '')) * 1000 # convert k to a thousand\n",
    "    if 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            total_stars = float(x.replace('M', '')) * 1000000 # convert M to a million\n",
    "    return int(total_stars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3ChnQMH-igSW",
    "outputId": "d55f2a76-b2b8-4f92-a4cd-05604a8049ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: Mean of empty slice.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"\n",
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  import sys\n",
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if (row['views'] =='0'):\n",
    "        continue\n",
    "    df.loc[index, \"views\"] = pd.to_numeric(ast.literal_eval(row['views']), errors='coerce').mean()\n",
    "    df.loc[index, \"likes\"] = pd.to_numeric(ast.literal_eval(row['likes']), errors='coerce').mean()\n",
    "    df.loc[index, \"dislikes\"] = pd.to_numeric(ast.literal_eval(row['dislikes']), errors='coerce').mean()\n",
    "    df.loc[index, \"CommentCount\"] = pd.to_numeric(ast.literal_eval(row['CommentCount']), errors='coerce').mean()\n",
    "    df.loc[index, \"subno\"] = np.mean([convert_si_to_number(number) for number in ast.literal_eval(row['subno'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "firydJ4B8P4t",
    "outputId": "0aff7799-e34d-4b4a-919c-a7552687f3d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Applied Sciences',\n",
       " 'Health Sciences',\n",
       " 'Environmental Sciences',\n",
       " 'General',\n",
       " 'Management Sciences',\n",
       " 'Life Sciences',\n",
       " 'Social Sciences']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = list(set(subjects.values()))\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-A8SDwcitvjD"
   },
   "outputs": [],
   "source": [
    "data = df.loc[:,['cited_by_gplus_count', 'cited_by_wikipedia_count', 'cited_by_rdts_count',\n",
    "       'Video mentions', 'cited_by_fbwalls_count','cited_by_tweeters_count','views', 'likes', 'dislikes', 'CommentCount', 'subno', 'Number of Dimensions citations', 'cited_by_feeds_count',\n",
    "       'cited_by_msm_count', 'cited_by_posts_count', 'Response_Rate', 'News mentions', 'Blog mentions'] ]\n",
    "target = df.loc[:, sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "pSVMYfsQGhJn",
    "outputId": "fea65ec5-ba59-498d-e1ce-153b0954c80f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    64402\n",
       "0    17484\n",
       "Name: Life Sciences, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.loc[:, 'Life Sciences'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KtcDDNrz1M3"
   },
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "7eTiM-DLzdeN",
    "outputId": "9608b341-4429-4758-822e-5a56b159dd25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in c:\\users\\srika\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOi3UGp1uADl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRiQFVvpt9uF"
   },
   "outputs": [],
   "source": [
    "traindata,testdata,traintarget,testtarget = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "#feature scaling\n",
    "sc = StandardScaler()\n",
    "traindata = sc.fit_transform(traindata)\n",
    "testdata = sc.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chgxdy5wwqoa"
   },
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "lr = LogisticRegression()\n",
    "mn = MultinomialNB()\n",
    "random = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BgIcU-kxeW4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZXXpjKfxfv-"
   },
   "outputs": [],
   "source": [
    "def print_score(y_pred, clf):\n",
    "    print(\"Clf: \", clf.__class__.__name__)\n",
    "    print(\"Hamming loss: {}\".format(hamming_loss(testdata, y_pred)))\n",
    "    print(\"Hamming score: {}\".format(hamming_score(testdata, y_pred)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oeu8tKP16C3a"
   },
   "source": [
    "**Logistic Regression with BinaryRelevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lH0LCXDixK-b",
    "outputId": "3d8fe971-9c35-48c3-f042-078ece6383f3"
   },
   "outputs": [],
   "source": [
    "classifier = BinaryRelevance(\n",
    "    classifier = lr,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "classifier.fit(traindata, traintarget)\n",
    "y_pred = classifier.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Qk6vrfZf1fxU",
    "outputId": "52aed2fc-bb30-4051-a697-43b17b6f7dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance F1-score: 0.716\n",
      "Binary Relevance Hamming Loss: 0.145\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "br_f1=metrics.f1_score(testtarget, y_pred, average='micro')\n",
    "br_hamm=metrics.hamming_loss(testtarget,y_pred)\n",
    "print('Binary Relevance F1-score:',round(br_f1,3))\n",
    "print('Binary Relevance Hamming Loss:',round(br_hamm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4813892145369285"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(testtarget, y_pred, normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7TtsxI-5659"
   },
   "source": [
    "**Random Forest with LabelPowerset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "idXh5pgd2j8A",
    "outputId": "0d068426-c097-4290-863e-231ae2c82e06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelPowerset(classifier=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       require_dense=[False, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "classifier = LabelPowerset(\n",
    "    classifier = random,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "classifier.fit(traindata, traintarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "a2CDxRK_26hL",
    "outputId": "57a9a0e4-39c9-498b-c1f3-7ca6cffa5d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.73\n",
      "Label Powerset Hamming Loss: 0.137\n"
     ]
    }
   ],
   "source": [
    "y_pred_random = classifier.predict(testdata)\n",
    "lp_f1=metrics.f1_score(testtarget, y_pred_random, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_random)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tXMhdXG_3VeZ",
    "outputId": "3fcc7756-f7b1-4432-c134-63af84d46701"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5275986713559985"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(testtarget, y_pred_random,normalize=True, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IaIUwN1c6Mfk"
   },
   "source": [
    "**Logistic Regression with one Vs rest approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "PAtYwF5Y4a5d",
    "outputId": "951ae4e6-0148-42dc-df5b-08d0ac3488f7"
   },
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(lr)\n",
    "clf.fit(traindata, traintarget)\n",
    "y_pred_lr = clf.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EL2QhAad5S-h",
    "outputId": "6f343ae9-21d6-4d76-ebcd-023f44cda001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.716\n",
      "Label Powerset Hamming Loss: 0.145\n"
     ]
    }
   ],
   "source": [
    "lp_f1=metrics.f1_score(testtarget, y_pred_lr, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_lr)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kjfTFOy4wvz"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WA5jct9y6vmp"
   },
   "outputs": [],
   "source": [
    "clf_dt = OneVsRestClassifier(decision)\n",
    "clf_dt.fit(traindata, traintarget)\n",
    "y_pred_dt = clf.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oFrB8utYJ9Va",
    "outputId": "147c3b77-5b48-457a-a7b7-0c53e64f6e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.716\n",
      "Label Powerset Hamming Loss: 0.145\n"
     ]
    }
   ],
   "source": [
    "lp_f1=metrics.f1_score(testtarget, y_pred_dt, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_dt)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "5kFNG63NKLkv",
    "outputId": "d5e2be37-3431-4c69-9b9c-3334af873a27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelPowerset(classifier=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "       require_dense=[False, True])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LabelPowerset(\n",
    "    classifier = decision,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "classifier.fit(traindata, traintarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7bQZxTVYKYEb",
    "outputId": "54f471c7-9cf2-4afd-9bc2-26d5f4df00a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.639\n",
      "Label Powerset Hamming Loss: 0.179\n"
     ]
    }
   ],
   "source": [
    "y_pred_decision = classifier.predict(testdata)\n",
    "lp_f1=metrics.f1_score(testtarget, y_pred_decision, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_decision)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPoqNlkgLi05"
   },
   "source": [
    "**Adding topic modelling of abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGiBm4nWLqH8"
   },
   "outputs": [],
   "source": [
    "df_abs = df[df.abstract != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7xH27a7HMkmA",
    "outputId": "2f96d0c6-b7d3-4b1f-d538-b7625112ca1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51377"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcHotYWyMoo0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srika\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import models\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 're', 'edu', 'use', 'new', 'may', 'could', 'say', 'get', 'go',\n",
    "                   'know', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can', 'also', 'many', \n",
    "                   'do', 'be', 'also'])\n",
    "stop_words = list(dict.fromkeys(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:24: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:27: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:24: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:27: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:24: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:27: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-37-9d6fa5d492ea>:16: DeprecationWarning: invalid escape sequence \\w\n",
      "  link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
      "<ipython-input-37-9d6fa5d492ea>:24: DeprecationWarning: invalid escape sequence \\S\n",
      "  summaries = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in summaries]\n",
      "<ipython-input-37-9d6fa5d492ea>:27: DeprecationWarning: invalid escape sequence \\s\n",
      "  summaries = [re.sub('\\s+', ' ', sent) for sent in summaries]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "def clean_data(summaries):\n",
    "    # Remove Emails\n",
    "    summaries = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in summaries]\n",
    "\n",
    "    # Remove new line characters\n",
    "    summaries = [re.sub('\\s+', ' ', sent) for sent in summaries]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    summaries = [re.sub(\"\\'\", \"\", sent) for sent in summaries]\n",
    "    \n",
    "    summaries = [strip_links(sent) for sent in summaries]\n",
    "\n",
    "    summaries = [strip_all_entities(sent) for sent in summaries]\n",
    "    \n",
    "    summaries = [re.sub(r\"(^|\\W)\\d+\", \"\", sent) for sent in summaries]\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = df_abs.abstract.values.tolist()\n",
    "\n",
    "summaries = clean_data(summaries)\n",
    "\n",
    "summaries_to_words = list(sent_to_words(summaries))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(summaries_to_words, min_count=5, threshold=100) # higher threshold fewer phrases. \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['massage', 'popular', 'complementary', 'alternative', 'medical', 'cam', 'treatment', 'anxiety', 'effectiveness', 'never', 'rigorously', 'evaluate', 'diagnose', 'anxiety', 'disorder', 'study', 'evaluate', 'effectiveness', 'therapeutic', 'massage', 'person', 'generalized_anxiety']\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "summaries_words_nostops = remove_stopwords(summaries_to_words)\n",
    "\n",
    "# Form Bigrams\n",
    "summaries_words_bigrams = make_bigrams(summaries_words_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "summaries_lemmatized = lemmatization(summaries_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(summaries_lemmatized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"increase\" + 0.016*\"diet\" + 0.016*\"level\" + 0.013*\"effect\" + '\n",
      "  '0.012*\"high\" + 0.012*\"intake\" + 0.011*\"fat\" + 0.011*\"body\" + '\n",
      "  '0.011*\"protein\" + 0.010*\"low\"'),\n",
      " (1,\n",
      "  '0.031*\"group\" + 0.019*\"exercise\" + 0.013*\"muscle\" + 0.012*\"week\" + '\n",
      "  '0.012*\"training\" + 0.010*\"increase\" + 0.010*\"study\" + 0.010*\"compare\" + '\n",
      "  '0.010*\"effect\" + 0.010*\"placebo\"'),\n",
      " (2,\n",
      "  '0.017*\"study\" + 0.014*\"patient\" + 0.011*\"cancer\" + 0.010*\"evidence\" + '\n",
      "  '0.010*\"include\" + 0.009*\"review\" + 0.009*\"treatment\" + 0.009*\"clinical\" + '\n",
      "  '0.009*\"effect\" + 0.008*\"risk\"'),\n",
      " (3,\n",
      "  '0.016*\"food\" + 0.014*\"consumption\" + 0.011*\"study\" + 0.009*\"effect\" + '\n",
      "  '0.008*\"use\" + 0.008*\"high\" + 0.007*\"intake\" + 0.007*\"extract\" + '\n",
      "  '0.007*\"concentration\" + 0.007*\"vitamin\"'),\n",
      " (4,\n",
      "  '0.013*\"brain\" + 0.007*\"study\" + 0.006*\"behavior\" + 0.006*\"show\" + '\n",
      "  '0.006*\"response\" + 0.006*\"social\" + 0.005*\"result\" + 0.005*\"cognitive\" + '\n",
      "  '0.005*\"memory\" + 0.005*\"suggest\"'),\n",
      " (5,\n",
      "  '0.039*\"cell\" + 0.010*\"mouse\" + 0.010*\"induce\" + 0.009*\"protein\" + '\n",
      "  '0.008*\"activity\" + 0.007*\"effect\" + 0.007*\"mechanism\" + 0.007*\"expression\" '\n",
      "  '+ 0.007*\"tissue\" + 0.006*\"receptor\"'),\n",
      " (6,\n",
      "  '0.020*\"gene\" + 0.012*\"human\" + 0.011*\"genetic\" + 0.008*\"dna\" + '\n",
      "  '0.007*\"infection\" + 0.007*\"sequence\" + 0.006*\"virus\" + 0.006*\"mutation\" + '\n",
      "  '0.006*\"host\" + 0.006*\"male\"'),\n",
      " (7,\n",
      "  '0.019*\"patient\" + 0.014*\"risk\" + 0.014*\"study\" + 0.013*\"age\" + 0.012*\"year\" '\n",
      "  '+ 0.011*\"associate\" + 0.008*\"child\" + 0.008*\"high\" + 0.008*\"woman\" + '\n",
      "  '0.007*\"association\"'),\n",
      " (8,\n",
      "  '0.011*\"use\" + 0.006*\"model\" + 0.006*\"system\" + 0.006*\"method\" + '\n",
      "  '0.006*\"high\" + 0.005*\"surface\" + 0.005*\"structure\" + 0.004*\"image\" + '\n",
      "  '0.004*\"base\" + 0.004*\"time\"'),\n",
      " (9,\n",
      "  '0.012*\"specie\" + 0.008*\"population\" + 0.005*\"early\" + 0.005*\"human\" + '\n",
      "  '0.005*\"suggest\" + 0.004*\"evolution\" + 0.004*\"year\" + 0.004*\"large\" + '\n",
      "  '0.004*\"datum\" + 0.004*\"group\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(summaries_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = summaries_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=id2word, passes=4, alpha=\"auto\")\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [lda_model[corpus[i]] for i in range(len(summaries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_document_to_dataframe(topics_document, num_topics):\n",
    "    res = pd.DataFrame(columns=range(num_topics))\n",
    "    for topic_weight in topics_document:\n",
    "        res.loc[0, topic_weight[0]] = topic_weight[1]\n",
    "    return res\n",
    "\n",
    "document_topic = pd.concat([topics_document_to_dataframe(topics_document, 10) for topics_document in topics]).reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract Topic 0</th>\n",
       "      <th>Abstract Topic 1</th>\n",
       "      <th>Abstract Topic 2</th>\n",
       "      <th>Abstract Topic 3</th>\n",
       "      <th>Abstract Topic 4</th>\n",
       "      <th>Abstract Topic 5</th>\n",
       "      <th>Abstract Topic 6</th>\n",
       "      <th>Abstract Topic 7</th>\n",
       "      <th>Abstract Topic 8</th>\n",
       "      <th>Abstract Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341237</td>\n",
       "      <td>0.060375</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.887903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080209</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.063890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150286</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515978</td>\n",
       "      <td>0.058540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.118319</td>\n",
       "      <td>0.060519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035502</td>\n",
       "      <td>0.866603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084281</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abstract Topic 0  Abstract Topic 1  Abstract Topic 2  Abstract Topic 3  \\\n",
       "0          0.000000          0.000000          0.453986               0.0   \n",
       "1          0.000000          0.000000          0.018708               0.0   \n",
       "2          0.063890          0.000000          0.179526               0.0   \n",
       "3          0.000000          0.000000          0.000000               0.0   \n",
       "4          0.035502          0.866603          0.000000               0.0   \n",
       "\n",
       "   Abstract Topic 4  Abstract Topic 5  Abstract Topic 6  Abstract Topic 7  \\\n",
       "0          0.000000          0.107188               0.0          0.341237   \n",
       "1          0.887903          0.000000               0.0          0.000000   \n",
       "2          0.592419          0.000000               0.0          0.000000   \n",
       "3          0.515978          0.058540               0.0          0.238730   \n",
       "4          0.000000          0.000000               0.0          0.000000   \n",
       "\n",
       "   Abstract Topic 8  Abstract Topic 9  \n",
       "0          0.060375          0.000000  \n",
       "1          0.080209          0.000000  \n",
       "2          0.150286          0.000000  \n",
       "3          0.118319          0.060519  \n",
       "4          0.084281          0.000000  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic.set_axis([\"Abstract Topic \"+ str(i) for i in range(10) ], axis='columns', inplace=True)\n",
    "document_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abs.index = pd.RangeIndex(len(df_abs.index))\n",
    "df_merged = df_abs.merge(document_topic, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['views', 'likes', 'dislikes', 'CommentCount', 'link', 'title',\n",
       "       'subname', 'subno', 'pubdate', 'description', 'Category',\n",
       "       'altmetric_id', 'cited_by_fbwalls_count', 'cited_by_gplus_count',\n",
       "       'cited_by_rdts_count', 'cited_by_tweeters_count',\n",
       "       'cited_by_videos_count', 'Video mentions',\n",
       "       'Number of Dimensions citations', 'abstract', 'alt_title',\n",
       "       'publisher_subjects', 'cited_by_feeds_count', 'cited_by_msm_count',\n",
       "       'cited_by_posts_count', 'cited_by_wikipedia_count',\n",
       "       'cited_by_accounts_count', 'subjects', 'Citations', 'number_of_days',\n",
       "       'avg_like_dislike_ratio', 'Response_Rate', 'Facebook mentions',\n",
       "       'Twitter mentions', 'News mentions', 'Blog mentions', 'scopus_subjects',\n",
       "       'Applied Sciences', 'Health Sciences', 'Environmental Sciences',\n",
       "       'General', 'Management Sciences', 'Life Sciences', 'Social Sciences',\n",
       "       'Abstract Topic 0', 'Abstract Topic 1', 'Abstract Topic 2',\n",
       "       'Abstract Topic 3', 'Abstract Topic 4', 'Abstract Topic 5',\n",
       "       'Abstract Topic 6', 'Abstract Topic 7', 'Abstract Topic 8',\n",
       "       'Abstract Topic 9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_merged.loc[:,['cited_by_gplus_count', 'cited_by_wikipedia_count', 'cited_by_rdts_count',\n",
    "       'Video mentions', 'cited_by_fbwalls_count','cited_by_tweeters_count','views', 'likes', 'dislikes', 'CommentCount', 'subno', 'Number of Dimensions citations', 'cited_by_feeds_count',\n",
    "       'cited_by_msm_count', 'cited_by_posts_count', 'Response_Rate', 'News mentions', 'Blog mentions', 'Abstract Topic 0', 'Abstract Topic 1', 'Abstract Topic 2',\n",
    "       'Abstract Topic 3', 'Abstract Topic 4', 'Abstract Topic 5','Abstract Topic 6', 'Abstract Topic 7', 'Abstract Topic 8',\n",
    "       'Abstract Topic 9'] ]\n",
    "target = df_merged.loc[:, sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,testdata,traintarget,testtarget = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "#feature scaling\n",
    "sc = StandardScaler()\n",
    "traindata = sc.fit_transform(traindata)\n",
    "testdata = sc.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.849\n",
      "Label Powerset Hamming Loss: 0.078\n",
      "accuracy score:  0.671\n"
     ]
    }
   ],
   "source": [
    "classifier = LabelPowerset(\n",
    "    classifier = random,\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "classifier.fit(traindata, traintarget)\n",
    "y_pred_random = classifier.predict(testdata)\n",
    "lp_f1=metrics.f1_score(testtarget, y_pred_random, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_random)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))\n",
    "print('accuracy score: ', round(metrics.accuracy_score(testtarget, y_pred_random), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Powerset F1-score: 0.849\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_random = classifier.predict(traindata)\n",
    "lp_f1_train=metrics.f1_score(traintarget, y_pred_train_random, average='micro')\n",
    "print('Training Label Powerset F1-score:',round(lp_f1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset F1-score: 0.839\n",
      "Label Powerset Hamming Loss: 0.08\n",
      "accuracy score:  0.625\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(lr)\n",
    "clf.fit(traindata, traintarget)\n",
    "y_pred_lr = clf.predict(testdata)\n",
    "lp_f1=metrics.f1_score(testtarget, y_pred_lr, average='micro')\n",
    "lp_hamm=metrics.hamming_loss(testtarget, y_pred_lr)\n",
    "print('Label Powerset F1-score:',round(lp_f1,3))\n",
    "print('Label Powerset Hamming Loss:',round(lp_hamm,3))\n",
    "print('accuracy score: ', round(metrics.accuracy_score(testtarget, y_pred_lr), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.13      0.22      1071\n",
      "          1       0.83      0.84      0.83      8499\n",
      "          2       0.00      0.00      0.00       329\n",
      "          3       0.64      0.25      0.36       971\n",
      "          4       0.00      0.00      0.00         5\n",
      "          5       0.92      0.98      0.95     11434\n",
      "          6       0.49      0.09      0.16       869\n",
      "\n",
      "avg / total       0.83      0.81      0.80     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testtarget, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.18      0.29      1071\n",
      "          1       0.79      0.92      0.85      8499\n",
      "          2       0.17      0.00      0.01       329\n",
      "          3       0.57      0.53      0.55       971\n",
      "          4       0.00      0.00      0.00         5\n",
      "          5       0.93      0.97      0.95     11434\n",
      "          6       0.48      0.12      0.19       869\n",
      "\n",
      "avg / total       0.83      0.85      0.82     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testtarget, y_pred_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "predicting_subjects_multilabel_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
